{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP:Assignment 1 Engine Search\n",
    "Name: Sitthiwat Damrongpreechar <br>\n",
    "Student ID: st123994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.8\n"
     ]
    }
   ],
   "source": [
    "# Check Python version\n",
    "! python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.24.4', '2.1.0+cu118')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__,torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\prasi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prasi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk and download reuters corpus\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the corpus\n",
    "from nltk.corpus import reuters\n",
    "#reuters.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1720901"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of words in the corpus\n",
    "len(reuters.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_full =[reuters.words(i).lower() for i in reuters.fileids()] \n",
    "corpus_full = reuters.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the number of sentences to 4500\n",
    "corpus = corpus_full[:4500]\n",
    "corpus = [[i.lower() for i in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Numeralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9250"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the unique words\n",
    "flatten = lambda l : [item for sublist in l for item in sublist]\n",
    "# Assign unique integer to each word\n",
    "vocabs = list(set(flatten(corpus)))\n",
    "len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9250"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create handy mapping between integer and word\n",
    "word2index= {v:idx for idx,v in enumerate(vocabs)}\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9251"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding the case of unknown word\n",
    "vocabs.append(\"<UNK>\")\n",
    "word2index['<UNK>'] = len(word2index)-1\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9250"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word = {v:k for k,v in word2index.items()}\n",
    "len(index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Word2Vec and Word2Vec(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for training\n",
    "def prepare_sequence(seq,word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pairs of center word, and outside word\n",
    "def random_batch(batch_size,corpus, window_size=2):\n",
    "    skipgram=[]\n",
    "    # loop each corpus (1 corpus)\n",
    "    # loop each document \n",
    "    for doc in corpus:\n",
    "        # look from the 3nd word until third last word\n",
    "        for i in range(window_size, len(doc) - window_size):\n",
    "            # center word\n",
    "            center = word2index[doc[i]]\n",
    "            # outside word = 4 words\n",
    "            outside = (word2index[doc[i-2]],word2index[doc[i-1]],word2index[doc[i+1]],word2index[doc[i+2]]) \n",
    "            # for each of these two outside words, we gonna append to alist\n",
    "            for each_out in outside:\n",
    "                # center, outside1; center outside2; center outside3; center outside4\n",
    "                skipgram.append([center,each_out])\n",
    "\n",
    "    random_index = np.random.choice(range(len(skipgram)),batch_size, replace=False)\n",
    "    input, label = [],[]\n",
    "    for index in random_index:\n",
    "        input.append([skipgram[index][0]])\n",
    "        label.append([skipgram[index][1]])\n",
    "\n",
    "    return np.array(input),np.array(label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the unigram table for negative sampling\n",
    "def unigramtable(corpus):\n",
    "    z=0.001\n",
    "    vocabs = list(set(flatten(corpus)))\n",
    "    # count the frequency of each word\n",
    "    word_count = Counter(flatten(corpus))\n",
    "    # get the total of words\n",
    "    num_total_words = sum([c for w,c in word_count.items()])\n",
    "    unigram_table = []\n",
    "    for word in vocabs:\n",
    "        uw = word_count[word] / num_total_words\n",
    "        uw_alpha = (uw **0.75) / z\n",
    "        unigram_table.extend([word] * int(uw_alpha))\n",
    "\n",
    "    return unigram_table\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the ramdom batch for glove\n",
    "def random_batch_glove(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    random_inputs, random_labels, random_coocs, random_weightings = [],[],[],[]\n",
    "    \n",
    "    # conver our skipgrams to id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]],word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    # randomply choose indexes based on batch size \n",
    "    random_index = np.random.choice(range(len(skip_grams_id)),batch_size,replace=False)\n",
    "    \n",
    "    # get the random in input and labels\n",
    "    for index in random_index:\n",
    "        random_inputs.append([skip_grams_id[index][0]])\n",
    "        random_labels.append([skip_grams_id[index][1]])\n",
    "\n",
    "        # coos\n",
    "        pair = skip_grams[index] # e.g., ('banana', 'fruit')\n",
    "        try: # if the co-occurences is available\n",
    "            cooc = X_ik[pair] # e.g., 3\n",
    "        except: # if not available, set to 1\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "\n",
    "        # weightings\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "\n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Build Co-occurance Matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the skipgram for glove\n",
    "from collections import Counter\n",
    "window_size=2\n",
    "X_i = Counter (flatten(corpus))\n",
    "skipgrams_glove = []\n",
    "\n",
    "for doc in corpus:\n",
    "    for i in range(2,len(doc)- window_size):\n",
    "        center = doc[i]\n",
    "        outside = [doc[i-2],doc[i-1],doc[i+1],doc[i+2]]\n",
    "        for each_out in outside:\n",
    "            skipgrams_glove.append((center,each_out))\n",
    "\n",
    "X_ik_skipgram_glove = Counter(skipgrams_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Weighting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighting function\n",
    "def weighting(w_i,w_j, X_ik):\n",
    "    # check whether the co-occurences between w_i and w_j is available.\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i,w_j)]\n",
    "    # if not exist, then set to 1 (\"laplace smoothing\")\n",
    "    except:\n",
    "        x_ij = 1\n",
    "    # set xmax = 100\n",
    "    x_max = 100\n",
    "    # set alpha = 0.75\n",
    "    alpha = 0.75\n",
    "    # if co-occurences does not exceed xmax, then just multiply with some alpha.\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij / x_max)**alpha \n",
    "    # otherwise, set to 1.\n",
    "    else:\n",
    "        result = 1\n",
    "    return result #weighting_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #keeping the co-occurences\n",
    "weighting_dic = {} #keeping the weighting function result\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs,2):\n",
    "    # if the pair exist in our copus\n",
    "    if X_ik_skipgram_glove.get(bigram):\n",
    "        co = X_ik_skipgram_glove[bigram]\n",
    "        X_ik[bigram] = co + 1 # for stability\n",
    "        X_ik[(bigram[1],bigram[0])] = co + 1 # for symmetry ex. apple,banana = banana,apple\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    weighting_dic[bigram] = weighting(bigram[0],bigram[1],X_ik)\n",
    "    weighting_dic[(bigram[1],bigram[0])] = weighting(bigram[1],bigram[0],X_ik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Skipgram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# Word2Vec Model\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self,voc_size,emb_size):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_center =nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside =nn.Embedding(voc_size, emb_size)\n",
    "\n",
    "    def forward (self, center, outside, all_vocabs):\n",
    "        center_embedding = self.embedding_center(center) # (batch_size, 1, emb_size)\n",
    "        outside_embedding = self.embedding_outside(outside) # (batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_outside(all_vocabs) # (batch_size, voc_size, emb_size)\n",
    "        assert center.size(0) == outside.size(0) == all_vocabs.size(0), \"Batch size mismatch\"\n",
    "        top_term =torch.exp(outside_embedding.bmm(center_embedding.transpose(1,2)).squeeze(2))\n",
    "        # (batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) \n",
    "        # => (batch_size, 1, 1).squeeze(2) = (batch_size, 1)\n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1,2)).squeeze(2)\n",
    "        # (batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) \n",
    "        # => (batch_size, voc_size, 1).squeeze(2) = (batch_size, voc_size)\n",
    "\n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1) #(batch_size,1)\n",
    "\n",
    "        loss = -torch.mean(torch.log(top_term/lower_term_sum)) #scalar\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Skipgram model and method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram: Skipgram(\n",
      "  (embedding_center): Embedding(9251, 2)\n",
      "  (embedding_outside): Embedding(9251, 2)\n",
      ")\n",
      "Input: torch.Size([2, 1]), Output: torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "test_x,test_y=random_batch(2,corpus)\n",
    "model = Skipgram(len(vocabs),2)\n",
    "print(\"Skipgram:\",model)\n",
    "test_input_tensor = torch.LongTensor(test_x)\n",
    "test_label_tensor = torch.LongTensor(test_y)\n",
    "assert torch.max(test_input_tensor) < len(vocabs), \"Invalid index in input_tensor\"\n",
    "assert torch.max(test_label_tensor) < len(vocabs), \"Invalid index in label_tensor\"\n",
    "print(f\"Input: {test_input_tensor.shape}, Output: {test_label_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Skipgram Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    negative_sampling =[]\n",
    "    for i in range(batch_size): # (1,k)\n",
    "        targets_index = targets[i].item()\n",
    "        nsample = []\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == targets_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        negative_sampling.append(prepare_sequence(nsample,word2index).reshape(1,-1))\n",
    "\n",
    "    return torch.cat(negative_sampling) #batch_size, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    def __init__(self,vocab_size,emb_size):\n",
    "        super(SkipgramNeg,self).__init__()\n",
    "        self.embedding_center = nn.Embedding(vocab_size,emb_size)\n",
    "        self.embedding_outside = nn.Embedding(vocab_size,emb_size)\n",
    "        self.log_sigmoid = nn.LogSigmoid()      \n",
    "\n",
    "    def forward(self,center,outside,negative_words):\n",
    "        #center, outside : (bs,1)\n",
    "        #negative_words : (bs,k)\n",
    "        center_emb = self.embedding_center(center) # (bs,1,emb_size)\n",
    "        outside_emb = self.embedding_outside(outside) # (bs,1,emb_size)\n",
    "        negative_emb = self.embedding_outside(negative_words) # (bs,k,emb_size)\n",
    "\n",
    "        uovc = outside_emb.bmm(center_emb.transpose(1,2)).squeeze(2) # (bs,1)\n",
    "        ukvc = -negative_emb.bmm(center_emb.transpose(1,2)).squeeze(2) # (bs,k)\n",
    "        ukvc_sum = torch.sum(ukvc,1).reshape(-1,1) # (bs,1)\n",
    "\n",
    "        loss = self.log_sigmoid(uovc) + self.log_sigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Skipgram Negative model and method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([7569]), tensor([1345, 5720, 3177, 5153, 6515]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "x,y = random_batch(batch_size,corpus)\n",
    "x_tensor = torch.LongTensor(x)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "k = 5\n",
    "unigram_table = unigramtable(corpus)\n",
    "neg_samples = negative_sampling(y_tensor,unigram_table,k)\n",
    "y_tensor[1],neg_samples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3362, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your model \n",
    "emb_size = 2\n",
    "vocab_size = len(vocabs)\n",
    "model = SkipgramNeg(vocab_size,emb_size)\n",
    "loss = model(x_tensor,y_tensor,neg_samples)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.embedding_center = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "\n",
    "        self.center_bias = nn.Embedding(voc_size, 1)\n",
    "        self.outside_bias = nn.Embedding(voc_size, 1)\n",
    "        \n",
    "\n",
    "    def forward(self,center,outside,cooc,weighting):\n",
    "        center_embeds = self.embedding_center(center) #(batch_size, 1, emb_size)\n",
    "        outside_embeds = self.embedding_outside(outside) #(batch_size, 1, emb_size)\n",
    "\n",
    "        center_bias = self.center_bias(center).squeeze(1) #(batch_size, k)\n",
    "        target_bias = self.outside_bias(outside).squeeze(1) #(batch_size, k)\n",
    "\n",
    "        # inner-product\n",
    "        inner_product = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) #(batch_size, 1)\n",
    "        # (batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) -> (batch_size, 1)\n",
    "\n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - cooc, 2)\n",
    "\n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing GloVe model and method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[7647],\n",
       "        [9233]]),\n",
       " array([[3259],\n",
       "        [6132]]),\n",
       " array([[1.38629436],\n",
       "        [2.30258509]]),\n",
       " array([[0.08944272],\n",
       "        [0.17782794]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "x, y, cooc, weighting = random_batch_glove(batch_size, corpus, skipgrams_glove, X_ik, weighting_dic)\n",
    "x,y,cooc,weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5232, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test our system\n",
    "model = Glove(voc_size=len(vocabs), emb_size=2)\n",
    "x_tensor = torch.LongTensor(x)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "cooc_tensor = torch.FloatTensor(cooc)\n",
    "weighting_tensor = torch.FloatTensor(weighting)\n",
    "loss = model(x_tensor, y_tensor, cooc_tensor, weighting_tensor)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 GloVe (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# you have to put this file in some python/gensime directory; just run it and it will inform where to put the file....\n",
    "glove_file = datapath('glove.6B.100d.txt') # search for this file in google and download it\n",
    "GloveGensimmodel = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('osaka', 0.7667967677116394)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gensim model already has most_similar function\n",
    "GloveGensimmodel.most_similar('tokyo',topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prasi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# prepare the parameters for training\n",
    "batch_size = 2 \n",
    "emb_size = 2 #50, 100, 200, 300\n",
    "voc_size = len(vocabs)\n",
    "k=5\n",
    "\n",
    "Skipgrammodel = Skipgram(voc_size,emb_size)\n",
    "SkipgramNegmodel = SkipgramNeg(vocab_size,emb_size)\n",
    "Glovemodel = Glove(voc_size,emb_size)\n",
    "\n",
    "critrion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_skipgram = optim.Adam(Skipgrammodel.parameters(),lr=0.001)\n",
    "optimizer_skipgramneg = optim.Adam(SkipgramNegmodel.parameters(),lr=0.001)\n",
    "optimizer_glove = optim.Adam(Glovemodel.parameters(),lr=0.001)\n",
    "\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9251])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape must be like (batch_size, voc_size)\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Training Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | cost: 8.888941 | Cumulative time: 5.731s\n",
      "Epoch: 20 | cost: 10.527663 | Cumulative time: 10.98s\n",
      "Epoch: 30 | cost: 10.591673 | Cumulative time: 16.29s\n",
      "Epoch: 40 | cost: 12.290310 | Cumulative time: 21.63s\n",
      "Epoch: 50 | cost: 10.649212 | Cumulative time: 27.01s\n",
      "Epoch: 60 | cost: 11.083242 | Cumulative time: 32.23s\n",
      "Epoch: 70 | cost: 10.446486 | Cumulative time: 37.45s\n",
      "Epoch: 80 | cost: 12.060201 | Cumulative time: 43.22s\n",
      "Epoch: 90 | cost: 7.732903 | Cumulative time: 49.44s\n",
      "Epoch: 100 | cost: 10.469149 | Cumulative time: 56.19s\n",
      "Training Skipgram Done!: 56.19s\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    input_batch, target_batch = random_batch(batch_size, corpus)\n",
    "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "\n",
    "    loss = Skipgrammodel(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    optimizer_skipgram.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer_skipgram.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if (epoch + 1)% 10==0:\n",
    "        end = time.time()\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | Cumulative time: {end-starttime :.4}s\")\n",
    "    \n",
    "print(f\"Training Skipgram Done!: {time.time()-starttime:.4}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Training Skipgram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | cost: 1.919172 | Cumulative time: 7.014s\n",
      "Epoch: 20 | cost: 1.771240 | Cumulative time: 13.05s\n",
      "Epoch: 30 | cost: 2.802083 | Cumulative time: 19.07s\n",
      "Epoch: 40 | cost: 1.789256 | Cumulative time: 25.4s\n",
      "Epoch: 50 | cost: 2.307091 | Cumulative time: 30.87s\n",
      "Epoch: 60 | cost: 3.047858 | Cumulative time: 36.71s\n",
      "Epoch: 70 | cost: 2.592919 | Cumulative time: 41.73s\n",
      "Epoch: 80 | cost: 1.216159 | Cumulative time: 47.05s\n",
      "Epoch: 90 | cost: 1.233761 | Cumulative time: 53.08s\n",
      "Epoch: 100 | cost: 1.330040 | Cumulative time: 58.31s\n",
      "Training Skipgram Negative Done!: 58.31s\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # get batch\n",
    "    input_batch, target_batch = random_batch(batch_size,corpus)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    target_tensor = torch.LongTensor(target_batch)\n",
    "\n",
    "    # predict\n",
    "    neg_samples = negative_sampling(target_tensor,unigram_table,k)\n",
    "    loss = SkipgramNegmodel(input_tensor,target_tensor,neg_samples)\n",
    "\n",
    "    # backpropagate\n",
    "    optimizer_skipgramneg.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer_skipgramneg.step()\n",
    "    \n",
    "    if (epoch + 1)% 10==0:\n",
    "        end = time.time()\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | Cumulative time: {end-starttime :.4}s\")\n",
    "    \n",
    "print(f\"Training Skipgram Negative Done!: {time.time()-starttime:.4}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Training GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | cost: 47.381474 | Cumulative time: 1.522s\n",
      "Epoch: 20 | cost: 12.463377 | Cumulative time: 3.022s\n",
      "Epoch: 30 | cost: 0.662771 | Cumulative time: 4.591s\n",
      "Epoch: 40 | cost: 3.059803 | Cumulative time: 6.57s\n",
      "Epoch: 50 | cost: 18.158466 | Cumulative time: 8.425s\n",
      "Epoch: 60 | cost: 1.316744 | Cumulative time: 10.34s\n",
      "Epoch: 70 | cost: 0.876961 | Cumulative time: 12.37s\n",
      "Epoch: 80 | cost: 0.101010 | Cumulative time: 14.28s\n",
      "Epoch: 90 | cost: 3.905553 | Cumulative time: 16.22s\n",
      "Epoch: 100 | cost: 0.721500 | Cumulative time: 18.06s\n",
      "Training GloVe Done!: 18.06s\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch_glove(batch_size, corpus, skipgrams_glove, X_ik, weighting_dic)\n",
    "    input_batch = torch.LongTensor(input_batch)          #(batch_size, 1)\n",
    "    target_batch = torch.LongTensor(target_batch)        #(batch_size, 1)\n",
    "    cooc_batch = torch.FloatTensor(cooc_batch)           #(batch_size, 1)\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch) #(batch_size, 1)\n",
    "\n",
    "    optimizer_glove.zero_grad()\n",
    "    loss = Glovemodel(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_glove.step()\n",
    "    \n",
    "    if(epoch + 1)% 10==0:\n",
    "        end = time.time()  \n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | Cumulative time: {end-starttime :.4}s\")\n",
    "    \n",
    "print(f\"Training GloVe Done!: {time.time()-starttime:.4}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Comparison and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Skip-gram, Skip-gram negative sampling, GloVe models on training loss, training time.\n",
    "| Model | Training Loss | Training Time |\n",
    "|----------|----------|----------|\n",
    "| Skip-gram | 10.469149 | 56.19s |\n",
    "|  Skip-gram negative sampling | 1.330040 | 58.31s |\n",
    "| GloVe | 0.721500  | 18.06s |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Word analogies dataset to calucalte between syntactic and semantic accuracy, similar to the methods in the Word2Vec and GloVe paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Download World Anology Dataset\n",
    "import requests\n",
    "\n",
    "url = \"https://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Save the content to a file\n",
    "    with open(\"word-test.v1.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(response.text)\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the dataset. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "with open(\"word-test.v1.txt\", \"r\") as file:\n",
    "    dataset = file.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "dataset = dataset.replace(\"\\t\", \"\")\n",
    "dataset = dataset.split(\": \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['Athens', 'Greece', 'Baghdad', 'Iraq'],\n",
       "  ['Athens', 'Greece', 'Bangkok', 'Thailand'],\n",
       "  ['Athens', 'Greece', 'Beijing', 'China'],\n",
       "  ['Athens', 'Greece', 'Berlin', 'Germany'],\n",
       "  ['Athens', 'Greece', 'Bern', 'Switzerland']],\n",
       " [['dancing', 'danced', 'decreasing', 'decreased'],\n",
       "  ['dancing', 'danced', 'describing', 'described'],\n",
       "  ['dancing', 'danced', 'enhancing', 'enhanced'],\n",
       "  ['dancing', 'danced', 'falling', 'fell'],\n",
       "  ['dancing', 'danced', 'feeding', 'fed']])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the categories\n",
    "capital_common_countries = [line.split(\" \") for line in dataset[1].split(\"\\n\")[1:-1]]\n",
    "past_tense = [line.split(\" \") for line in dataset[12].split(\"\\n\")[1:-1]]\n",
    "capital_common_countries[:5], past_tense[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Syntactic and Semantic Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get embedding of all words in vocabs\n",
    "def get_vocabs_embed(model, vocabs):\n",
    "    embeddings = {}\n",
    "\n",
    "    for word in vocabs:\n",
    "        try:\n",
    "            index = word2index[word]\n",
    "        except:\n",
    "            index = word2index[\"<UNK>\"]\n",
    "\n",
    "        word_tensor = torch.LongTensor([index])\n",
    "\n",
    "        emb_center = model.embedding_center(word_tensor)\n",
    "        emb_outside = model.embedding_outside(word_tensor)\n",
    "        emb = (emb_center + emb_outside) / 2\n",
    "\n",
    "        embeddings[word] = (emb[0][0].item(), emb[0][1].item())\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9251 9251 9251\n"
     ]
    }
   ],
   "source": [
    "# Embedding of Skipgram, Skipgram Negative, Glove\n",
    "Skipgram_embeddings = get_vocabs_embed(Skipgrammodel, vocabs)\n",
    "SkipgramNeg_embeddings = get_vocabs_embed(SkipgramNegmodel, vocabs)\n",
    "Glove_embeddings = get_vocabs_embed(Glovemodel, vocabs)\n",
    "print(len(Skipgram_embeddings), len(SkipgramNeg_embeddings), len(Glove_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get the cosine similarity\n",
    "def cosine_similarity(A,B):\n",
    "    dot_product = np.dot(A,B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B) \n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get the most similar word (Later used in Web App)\n",
    "def get_most_similar_word(embeddings, word_vec, type, topn=10):\n",
    "    similarities = {}\n",
    "\n",
    "    if type == \"cosine\":\n",
    "        for vocab, emb in embeddings.items():\n",
    "            similarities[vocab] = cosine_similarity(word_vec, np.array(emb))\n",
    "    elif type == \"dot\":\n",
    "        for vocab, emb in embeddings.items():\n",
    "            similarities[vocab] = np.dot(word_vec, np.array(emb))\n",
    "    else:\n",
    "        raise ValueError(\"Type must be either 'cosine' or 'dot'\")\n",
    "\n",
    "    similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similarities[1:topn+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get embedding of a word\n",
    "def get_single_embed(embbeding_model,word):\n",
    "    word = word.lower()\n",
    "    try: \n",
    "        index = word2index[word]\n",
    "    except: # for unknown word\n",
    "        word = '<UNK>'\n",
    "\n",
    "    return embbeding_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.821340799331665, -0.5290414690971375)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the unknown word\n",
    "word_vec_bkk = get_single_embed(Glove_embeddings,'Bangkok')\n",
    "word_vec_bkk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<UNK>', 0.9999999999999999),\n",
       " ('unsettled', 0.9999993256071421),\n",
       " ('nwor', 0.9999991022325584),\n",
       " ('winterhalter', 0.9999990348383608),\n",
       " ('picco', 0.9999990202466155),\n",
       " ('suggest', 0.9999984970948746),\n",
       " ('when', 0.9999983896774476),\n",
       " ('ownership', 0.9999982659472577),\n",
       " ('jobless', 0.9999979350807402),\n",
       " ('915', 0.9999971155957721)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the unknown word\n",
    "get_most_similar_word(Glove_embeddings, word_vec_bkk, \"cosine\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get the accuracy\n",
    "def get_accuracy(embeddings, dataset, model=\"normal\"):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for category in dataset:\n",
    "        total += 1\n",
    "        word_a, word_b, word_c, word_d = category[0].lower(), category[1].lower(), category[2].lower(), category[3].lower()\n",
    "        if model == \"normal\":\n",
    "            prediction = np.array(get_single_embed(embeddings, word_b)) - np.array(get_single_embed(embeddings, word_a)) + np.array(get_single_embed(embeddings, word_c))\n",
    "            predicted_word = get_most_similar_word(embeddings, prediction, \"cosine\", 1)[0][0]\n",
    "        elif model == \"gensim\":\n",
    "            predicted_word=embeddings.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)[0][0]\n",
    "\n",
    "        if predicted_word == word_d:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / total\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9387351778656127"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the accuracy of Glove Gensim\n",
    "get_accuracy(GloveGensimmodel, capital_common_countries, \"gensim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all the models accuracy\n",
    "skipgram_syntatic = get_accuracy(Skipgram_embeddings, capital_common_countries)\n",
    "skipgram_semantic = get_accuracy(Skipgram_embeddings, past_tense)\n",
    "skipgramneg_syntatic = get_accuracy(SkipgramNeg_embeddings, capital_common_countries)\n",
    "skipgramneg_semantic = get_accuracy(SkipgramNeg_embeddings, past_tense)\n",
    "glove_syntatic = get_accuracy(Glove_embeddings, capital_common_countries)\n",
    "glove_semantic = get_accuracy(Glove_embeddings, past_tense)\n",
    "gensim_syntatic = get_accuracy(GloveGensimmodel, capital_common_countries, \"gensim\")\n",
    "gensim_semantic = get_accuracy(GloveGensimmodel, past_tense, \"gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram Syntatic:  0.0\n",
      "Skipgram Semantic:  0.000641025641025641\n",
      "--------------------\n",
      "Skipgram Negative Syntatic:  0.0\n",
      "Skipgram Negative Semantic:  0.0\n",
      "--------------------\n",
      "Glove Syntatic:  0.0\n",
      "Glove Semantic:  0.0\n",
      "--------------------\n",
      "Gensim Syntatic:  0.9387351778656127\n",
      "Gensim Semantic:  0.5544871794871795\n"
     ]
    }
   ],
   "source": [
    "# Print the result\n",
    "print(\"Skipgram Syntatic: \", skipgram_syntatic)\n",
    "print(\"Skipgram Semantic: \", skipgram_semantic)\n",
    "print(\"--------------------\")\n",
    "print(\"Skipgram Negative Syntatic: \", skipgramneg_syntatic)\n",
    "print(\"Skipgram Negative Semantic: \", skipgramneg_semantic)\n",
    "print(\"--------------------\")\n",
    "print(\"Glove Syntatic: \", glove_syntatic)\n",
    "print(\"Glove Semantic: \", glove_semantic)\n",
    "print(\"--------------------\")\n",
    "print(\"Gensim Syntatic: \", gensim_syntatic)\n",
    "print(\"Gensim Semantic: \", gensim_semantic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model               | Window Size | Training Loss | Syntactic Accuracy (%) | Semantic Accuracy (%) |\n",
    "|---------------------|-------------|---------------|---------------------|-------------------|\n",
    "| Skipgram            | 2           |  10.469149    |         0           |         0.000641025641025641         |\n",
    "| Skipgram (NEG)      | 2           |  1.330040     |         0           |         0         |\n",
    "| Glove               | 2           |   0.721500    |         0           |           0       |\n",
    "| Glove (Gensim)      | -           |        -      |        93.8735             |       55.4487            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Correlation Similarity\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1     word2  similarity_score\n",
       "0         tiger       cat              7.35\n",
       "1         tiger     tiger             10.00\n",
       "2         plane       car              5.77\n",
       "3         train       car              6.31\n",
       "4    television     radio              6.77\n",
       "..          ...       ...               ...\n",
       "198     rooster    voyage              0.62\n",
       "199        noon    string              0.54\n",
       "200       chord     smile              0.54\n",
       "201   professor  cucumber              0.31\n",
       "202        king   cabbage              0.23\n",
       "\n",
       "[203 rows x 3 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the wordsim_similarity_goldstandard.txt\n",
    "import pandas as pd\n",
    "with open(\"./wordsim_similarity_goldstandard.txt\", \"r\") as file:\n",
    "    wsg = pd.read_csv(file, sep=\"\\t\", names=[\"word1\",\"word2\",\"similarity_score\"])\n",
    "wsg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the similarity dataset to find the correlation between models's dot product and the provided similarity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all embeddings to numpy array (for more efficient computation)\n",
    "Skipgram_embeddings_np = {key: np.array(value) for key, value in Skipgram_embeddings.items()}\n",
    "SkipgramNeg_embeddings_np = {key: np.array(value) for key, value in SkipgramNeg_embeddings.items()}\n",
    "Glove_embeddings_np = {key: np.array(value) for key, value in Glove_embeddings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "word1:  False\n",
      "word2:  True\n",
      "after\n",
      "word1:  False\n",
      "word2:  False\n"
     ]
    }
   ],
   "source": [
    "# Check the uppercase in the wordsim_similarity_goldstandard.txt columns\n",
    "print(\"before\")\n",
    "print(\"word1: \",any(wsg[\"word1\"].str.isupper()))\n",
    "print(\"word2: \",any(wsg[\"word2\"].str.isupper()))\n",
    "\n",
    "wsg['word1'] = wsg['word1'].str.lower()\n",
    "wsg['word2'] = wsg['word2'].str.lower()\n",
    "print(\"after\")\n",
    "print(\"word1: \",any(wsg[\"word1\"].str.isupper()))\n",
    "print(\"word2: \",any(wsg[\"word2\"].str.isupper()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the skipgram similarity score to the dataframe\n",
    "wsg[\"Skipgram\"] = wsg.apply(lambda x: np.dot(get_single_embed(Skipgram_embeddings,x['word1']),get_single_embed(SkipgramNeg_embeddings,x['word2'])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gram with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the skipgram negative similarity score to the dataframe\n",
    "wsg[\"NEG\"] = wsg.apply(lambda x: np.dot(get_single_embed(SkipgramNeg_embeddings,x['word1']),get_single_embed(SkipgramNeg_embeddings,x['word2'])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the glove similarity score to the dataframe\n",
    "wsg[\"GloVe\"] = wsg.apply(lambda x: np.dot(get_single_embed(Glove_embeddings,x['word1']),get_single_embed(Glove_embeddings,x['word2'])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.82157 ,  0.7952  ,  0.80816 , -0.09699 , -0.11838 ,  0.021151,\n",
       "        0.62663 , -0.62621 , -0.602   , -0.76326 , -0.29244 ,  1.0718  ,\n",
       "        1.0264  ,  0.51485 , -0.26717 ,  0.98914 , -0.13989 , -0.14031 ,\n",
       "        0.068456,  0.64535 ,  0.25223 , -0.32911 ,  0.16927 ,  0.38475 ,\n",
       "        0.41674 ,  1.097   , -0.66572 , -0.64471 , -0.12801 , -0.55854 ,\n",
       "       -0.36549 ,  0.079815,  0.17361 ,  0.13237 ,  1.0031  , -0.50629 ,\n",
       "       -0.84352 ,  0.8185  ,  0.41386 , -0.17179 , -0.49508 ,  0.61723 ,\n",
       "        0.55838 ,  0.36077 ,  0.34123 ,  0.13034 , -0.11284 ,  0.4226  ,\n",
       "       -0.67964 , -0.66582 , -1.1275  ,  0.16545 ,  0.028074,  0.59515 ,\n",
       "        0.40081 , -1.5653  , -0.38159 ,  0.16342 ,  0.81675 ,  0.43633 ,\n",
       "       -0.051347,  0.68191 , -0.66771 ,  0.94923 ,  0.24136 ,  0.64997 ,\n",
       "        0.24963 ,  0.41191 ,  0.19212 ,  1.1599  , -0.089726, -0.55557 ,\n",
       "       -0.64921 , -0.39078 , -0.65487 ,  0.031186,  0.073248,  0.45036 ,\n",
       "       -0.83119 ,  1.1082  ,  0.29897 , -0.83352 ,  0.20487 ,  0.19633 ,\n",
       "       -0.12631 ,  0.33189 , -0.16336 , -0.12867 ,  0.22577 ,  1.0092  ,\n",
       "        0.51273 , -0.30085 , -0.012987, -0.087924, -0.47341 ,  0.26818 ,\n",
       "       -0.82033 ,  0.37465 ,  0.73564 ,  0.11336 ], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Glove Gensim Embedding\n",
    "GloveGensimmodel['tiger']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the glove(gensim) similarity score to the dataframe\n",
    "wsg[\"GloVe(gensim)\"] = wsg.apply(lambda x: np.dot(GloveGensimmodel[x['word1']],GloveGensimmodel[x['word2']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>Skipgram</th>\n",
       "      <th>NEG</th>\n",
       "      <th>GloVe</th>\n",
       "      <th>GloVe(gensim)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>-0.013500</td>\n",
       "      <td>1.046953</td>\n",
       "      <td>-0.347262</td>\n",
       "      <td>15.629376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>-0.243499</td>\n",
       "      <td>4.224055</td>\n",
       "      <td>3.597167</td>\n",
       "      <td>32.800144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.218328</td>\n",
       "      <td>1.981475</td>\n",
       "      <td>24.047297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.218328</td>\n",
       "      <td>1.981475</td>\n",
       "      <td>25.472925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.371809</td>\n",
       "      <td>0.166541</td>\n",
       "      <td>0.189803</td>\n",
       "      <td>34.689987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-0.209303</td>\n",
       "      <td>3.022633</td>\n",
       "      <td>-0.921982</td>\n",
       "      <td>1.683646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.243499</td>\n",
       "      <td>4.224055</td>\n",
       "      <td>3.597167</td>\n",
       "      <td>1.070592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.243499</td>\n",
       "      <td>4.224055</td>\n",
       "      <td>3.597167</td>\n",
       "      <td>6.762520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-1.292310</td>\n",
       "      <td>0.294015</td>\n",
       "      <td>1.441104</td>\n",
       "      <td>-0.230552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.501412</td>\n",
       "      <td>1.963579</td>\n",
       "      <td>2.006315</td>\n",
       "      <td>1.400288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1     word2  similarity_score  Skipgram       NEG     GloVe  \\\n",
       "0         tiger       cat              7.35 -0.013500  1.046953 -0.347262   \n",
       "1         tiger     tiger             10.00 -0.243499  4.224055  3.597167   \n",
       "2         plane       car              5.77  0.001743  0.218328  1.981475   \n",
       "3         train       car              6.31  0.001743  0.218328  1.981475   \n",
       "4    television     radio              6.77  0.371809  0.166541  0.189803   \n",
       "..          ...       ...               ...       ...       ...       ...   \n",
       "198     rooster    voyage              0.62 -0.209303  3.022633 -0.921982   \n",
       "199        noon    string              0.54 -0.243499  4.224055  3.597167   \n",
       "200       chord     smile              0.54 -0.243499  4.224055  3.597167   \n",
       "201   professor  cucumber              0.31 -1.292310  0.294015  1.441104   \n",
       "202        king   cabbage              0.23  1.501412  1.963579  2.006315   \n",
       "\n",
       "     GloVe(gensim)  \n",
       "0        15.629376  \n",
       "1        32.800144  \n",
       "2        24.047297  \n",
       "3        25.472925  \n",
       "4        34.689987  \n",
       "..             ...  \n",
       "198       1.683646  \n",
       "199       1.070592  \n",
       "200       6.762520  \n",
       "201      -0.230552  \n",
       "202       1.400288  \n",
       "\n",
       "[203 rows x 7 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataframe\n",
    "wsg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the spearman correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_correlation, p_value_s = spearmanr(wsg['similarity_score'], wsg['Skipgram'])\n",
    "skipgramneg_correlation, p_value_sn = spearmanr(wsg['similarity_score'], wsg['NEG'])\n",
    "glove_correlation, p_value_g = spearmanr(wsg['similarity_score'], wsg['GloVe'])\n",
    "glovegensim_correlation, p_value_gs= spearmanr(wsg['similarity_score'], wsg['GloVe(gensim)'])\n",
    "ytruecorrelation, p_value_yt = spearmanr(wsg['similarity_score'], wsg['similarity_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram:  0.16847053383501415\n",
      "Skipgram Negative:  0.08786851950031294\n",
      "Glove:  -0.012886622587326878\n",
      "Glove Gensim:  0.5430870624672256\n",
      "Ytrue:  1.0\n"
     ]
    }
   ],
   "source": [
    "# print the result\n",
    "print(\"Skipgram: \", skipgram_correlation)\n",
    "print(\"Skipgram Negative: \", skipgramneg_correlation)\n",
    "print(\"Glove: \", glove_correlation)\n",
    "print(\"Glove Gensim: \", glovegensim_correlation)\n",
    "print(\"Ytrue: \", ytruecorrelation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model          | Skipgram |NEG|GloVe|GloVe(Gensim)|Y_true|\n",
    "|----------------|----------------------|---------------|---------------|---------------|---------------|\n",
    "| Spearman Correlation   | -0.06504503887196321 |0.04463185808838047 |-0.019138662738073235   |  0.5430870624672256  | 1.0  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all model as the pickle file\n",
    "import pickle\n",
    "with open('./A1_Engine_Search/code/SkipgramNeg_embeddings.pickle', 'wb') as handle:\n",
    "    pickle.dump(SkipgramNeg_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./A1_Engine_Search/code/Skipgram_embeddings.pickle', 'wb') as handle:\n",
    "    pickle.dump(Skipgram_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./A1_Engine_Search/code/Glove_embeddings.pickle', 'wb') as handle:\n",
    "    pickle.dump(Glove_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9251 9251 9251\n"
     ]
    }
   ],
   "source": [
    "# Check all the pickle files\n",
    "with open('./A1_Engine_Search/code/SkipgramNeg_embeddings.pickle', 'rb') as handle:\n",
    "    a = pickle.load(handle)\n",
    "with open('./A1_Engine_Search/code/Skipgram_embeddings.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "with open('./A1_Engine_Search/code/Glove_embeddings.pickle', 'rb') as handle:\n",
    "    c = pickle.load(handle)\n",
    "print(len(a), len(b), len(c))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
